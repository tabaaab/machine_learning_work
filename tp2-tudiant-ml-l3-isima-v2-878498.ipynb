{"metadata":{"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Introduction à knn\n---\nLe but de ce TP est de mieux appréhender l'apprentissage via une méthode type 'plus proches voisins'. \n--- \n\n## Conseils\n- Pour chaque question, essayer d'écrire des fonctions plutôt que des scripts\n- Eviter d'avoir deux fonctions qui portent le même nom (redéfinition de la fonction), préférer ajouter un paramètre de choix","metadata":{"_uuid":"eb4bfdf8a11a8189f15da6b32bd628e586a4676b"}},{"cell_type":"code","source":"# Placer ici vos imports pour la totalité du TP\nimport pandas as pd\npd.plotting.register_matplotlib_converters()\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nprint(\"Setup Complete\")\nimport numpy as np\n\nimport sklearn.model_selection as skl\nimport sklearn.metrics as skm\nimport sklearn.neighbors as skn\n\nimport os","metadata":{"_uuid":"85d6c55ff476232a92fe0b8d5e43248a573b5cb5","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# paramètres globaux\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---\n# Construction d'un jeu de données simple\n---","metadata":{}},{"cell_type":"markdown","source":"## Ecrire une fonction ```separation```\n- Entrée :  un couple (x, y) de flottants\n- Sortie : 'A' si y<f(x)  et 'B' sinon, où $f(x) = \\frac{x}{3}*\\left(2+\\cos\\left(\\frac{x}{6}\\right)\\right)$\n\n## Ecrire une fonction ```donneesSimples``` \n- Entrée : \n    - ```N``` un entier \n    - ```f``` une fonction prenant en entrée deux flottants\n- Sortie : un dataframe de ```N``` échantillons de 3 attributs :\n  - ```abscisse``` : tiré en aléatoire uniforme dans [0 ; 100], flottant\n  - ```ordonnee``` : tiré en aléatoire uniforme dans [0 ; 100], flottant\n  - ```classe = f(abscisse, ordonnée)```\n- vous *devez* utiliser un apply de la fonction séparation\n  \n## Ecrire une fonction ```representation```\nCette fonction génère une représentation graphique des données dans le plan\n\n## Créer une BD ```donnees``` de 300 données\n- par application de ```donneesSimples(300, separation)```\n\n\n## Représenter la base de donnée graphiquement","metadata":{}},{"cell_type":"code","source":"def separation(x, y):\n    if y < x/3*(2+np.cos(x/6)):\n        return 'A'\n    else:\n        return 'B'\n    \ndef donneesSimples(N, f):\n    def fct(data):\n        return f(data.Abscisse, data.Ordonnee)\n    \n    absc = np.random.rand(N)*100\n    ord = np.random.rand(N)*100\n    \n    data = pd.DataFrame({'Abscisse' : absc, 'Ordonnee' : ord})\n    data2 = data.apply(fct, 1)\n    \n    data3 = pd.DataFrame({'Abscisse' : data.Abscisse, 'Ordonnee' : data.Ordonnee, 'Classe' : data2})\n    \n    return data3\n\ndef representation(data):\n    sns.scatterplot('Abscisse', 'Ordonnee', 'Classe', data = data)\n    \n    plot2 = np.linspace(0, 100, 300)\n    sns.lineplot(plot2, plot2/3*(2+np.cos(plot2/6)))\n    plt.figure()\n    \n\ndonnees = donneesSimples(300, separation)\n                 \ndisplay(donnees.head())\nrepresentation(donnees)","metadata":{"_uuid":"0c575b848683ec56ab7e9a4b23161499fc007f31","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## En utilisant scikit-learn, séparer ```donnees``` en deux ensembles\nEcrire la fonction , fonction ```creerBases``` qui sépare la base de données en :\n- apprentissage (et validation) : 80%\n- test 20%  \n \n**NB** : l'ensemble d'apprentissage servira également d'ensemble de validation puisqu'on utilisera une validation croisée","metadata":{"_uuid":"1f8e66c76444474c61a98793a53d65af4392dfad"}},{"cell_type":"code","source":"def creerBases(data):\n    data_X = data.loc[:, ['Abscisse', 'Ordonnee']]\n    data_Y = data.loc[:, 'Classe']\n    train_X, test_X, train_Y, test_Y = skl.train_test_split(data_X, data_Y, test_size = 0.2, stratify=donnees.Classe)\n    return train_X, test_X, train_Y, test_Y\n\ndonnees_train_X, donnees_test_X, donnees_train_Y, donnees_test_Y = creerBases(donnees)","metadata":{"_uuid":"b11ef00aa86d0d6fdaf74aa3059dee18b0729b72","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Théorie et sa mise en pratique dans sklearn\n- Rappeler le fonctionnement de la classification par knn\n- Expliquer les variations proposées par 'algorithm'\n- Expliquer l'influence de 'p' dans la distance de Minkowski (exemples à l'appui)\n- Expliquer l'influence de 'weight' (exemples à l'appui)","metadata":{}},{"cell_type":"markdown","source":" Le principe de la classification par plus proche voisin est d'observer les k plus proches voisins (en terme de similitude des attributs) et de prendre la classe la plus représentées.\n\n    BallTree, KDTree, Brute, auto KNeighborsClassifier\n\n","metadata":{}},{"cell_type":"markdown","source":"# Représenter la surface de séparation en fonction du nombre de voisins\n## Fonction de coloration\nEcrire une fonction ```colorationPlan``` qui produit une représentation [0 ; 100]x[0 ; 100] colorée en fonction de la classe attribuée par votre classifieur.\nen fonction de la classe attribuée par votre classifieur à chacun de ses points.\n- Entrées : \n    - ```classifieur``` une fonction de classification\n    - ```NbPts``` nombre de points à prendre dans chaque direction pour discrétiser [0 ; 100]x[0 ; 100]\n- Affichage : la coloration du carré demandée en utilisant par exemple un ```scatterplot```\n\n## Observer et analyser l'influence du choix de 'k' sur la classification obtenue\nPour différentes valeurs de ```k``` dans le classifieur, utiliser la fonction ```colorationPlan``` en superposant sur le graphique la ligne de séparation théorique (l'équation du début) en utilisant par exemple un ```lineplot```\n","metadata":{}},{"cell_type":"code","source":"def colorationPlan(cla, NbPts):\n    data = pd.DataFrame({'Abscisse' : [], 'Ordonnee' : [], 'Classe' : []})\n    temp = 0\n    plot = np.linspace(0, 100, NbPts)\n    for i in plot:\n        for j in plot:\n            data.loc[temp] = [i, j, cla.predict([[i, j]])[0]]\n            temp+=1\n    representation(data)\n    \nfor i in range(1, 11, 1):\n    neigh = skn.KNeighborsClassifier(n_neighbors=i)\n    neigh.fit(donnees_train_X, donnees_train_Y)\n\n    colorationPlan(neigh, 50)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Commenter\n- Essayer d'expliquer les résultats obtenus\n- Entre deux résultats équivalents pour deux valeurs de 'k' différentes, lequel choisir ?","metadata":{"_uuid":"47cc30607750781e8bb9351ac418ce3a9ffe96d4"}},{"cell_type":"markdown","source":"Explications : \n\nOn a des résultats assez différent d'une valeur de k à une autre. Plus notre k sera grand, moins on aura de points orange car il y a une plus grande proportion de bleu que d'orange. Il faut donc réussir à trouver la valeur de k la plus élevée possible en évitant d'avoir un surplus de bleu lié à leur forte quantité.\n\n\nLa valeur de k la plus interessante semble être k = 4 car on vois sur le graphique que c'est le seul a respecter la partie gauche du graphique et donc à ne pas se faire trop influencer par le bleu au dessus de la courbe.","metadata":{"_uuid":"d9520535b70f6fbb3e0eb471de70894970db0106"}},{"cell_type":"markdown","source":"## Recherche des paramètres optimaux\n- utiliser une gridsearch pour déterminer les meilleurs paramètres, expliquer la valeur de cv\n- valider votre apprentissage sur l'ensemble de validation\n- ne pas oublier de comparer la qualité obtenue sur l'ensemble de test et sur l'ensemble de validation","metadata":{"_uuid":"377aed07229608137ddc91785751bebd0de09efe"}},{"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\n\ngrid_params = {\n    'n_neighbors': [2, 3, 4, 5, 6, 7, 8, 9, 10]\n}\n\ngrid_search = GridSearchCV(skn.KNeighborsClassifier(), grid_params, verbose = 1, cv = 3, n_jobs=-1)\nresult = grid_search.fit(donnees.iloc[:,:-1], donnees.iloc[:,-1])\n\ndisplay(result.best_params_)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"cv = validation en croix","metadata":{"trusted":true}},{"cell_type":"markdown","source":"# Passez temporairement le nombre d'échantillons de la base de données de départ à 100 puis à 700\n- Observer l'influence de la quantité de données sur les valeurs optimales des paramètres, expliquer\n- Observer l'influence de la quantité de données sur la qualité du résultat obtenu, expliquer\n- Observer l'influence de la quantité de données sur le temps de calcul, expliquer","metadata":{}},{"cell_type":"code","source":"donnees = donneesSimples(100, separation)\ndonnees_train_X, donnees_test_X, donnees_train_Y, donnees_test_Y = creerBases(donnees)\n\ngrid_search = GridSearchCV(skn.KNeighborsClassifier(), grid_params, verbose = 1, cv = 3, n_jobs=-1)\nresult = grid_search.fit(donnees.iloc[:,:-1], donnees.iloc[:,-1])\n\ndisplay(result.best_params_)\n\nneigh = skn.KNeighborsClassifier(n_neighbors=3)\nneigh.fit(donnees_train_X, donnees_train_Y)\n\ncolorationPlan(neigh, 100)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"donnees = donneesSimples(700, separation)\ndonnees_train_X, donnees_test_X, donnees_train_Y, donnees_test_Y = creerBases(donnees)\n\ngrid_search = GridSearchCV(skn.KNeighborsClassifier(), grid_params, verbose = 1, cv = 3, n_jobs=-1)\nresult = grid_search.fit(donnees.iloc[:,:-1], donnees.iloc[:,-1])\n\ndisplay(result.best_params_)\n\nneigh = skn.KNeighborsClassifier(n_neighbors=3)\nneigh.fit(donnees_train_X, donnees_train_Y)\n\ncolorationPlan(neigh, 200)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"......\n\nValeur optimale :  \nOn peut dire que le nombre de voisin optimal n'a rien a voir avec la taille des données.\n\nRésultat :  \nLe résultat est beaucoup plus précis en ajoutant plus de points même si j'ai l'impression que cela se sent vraiment quand on a des écart allant dans les centaines de points d'écart.\n\nTemps de calcul :  \nLe temps de calcul est plus long (voir même beaucoup plus long). Rien qu'avec le gridSearch on passe de moins d'une seconde à quelque secondes entre 100 et 700 données. Générer le graphique est cependant beaucoup plus long (d'pù le fait que je l'ai généré avec 200 points et non 700).\n","metadata":{}},{"cell_type":"markdown","source":"---\n# Effet du bruit\nDans cette partie on va examiner la résistance au bruit de l'algorithme, et son influence sur le choix du nombre de voisins.\n---","metadata":{"_kg_hide-input":false}},{"cell_type":"markdown","source":"## Création d'un bruit\nEtudier et expliquer le code suivant :\n```{.python}\ndef genere_separation_bruit(mu, sigma):\n  def separation_bruit(x,y ):\n    classe = 'A' if y<f(x) else 'B'\n    if abs(y-f(x)) <norm.ppf(np.random.rand(),mu, sigma):\n        classe = 'A' if classe == 'B' else 'B'\n    return classe\n  return lambda x,y: separation_bruit(x,y)\n```","metadata":{}},{"cell_type":"code","source":"def genere_separation_bruit(mu, sigma):\n    def separation_bruit(x,y ):\n        classe = 'A' if y<f(x) else 'B'\n        if abs(y-f(x)) <norm.ppf(np.random.rand(),mu, sigma):\n            classe = 'A' if classe == 'B' else 'B'\n        return classe\n    return lambda x,y: separation_bruit(x,y)\n\n\nf = genere_separation_bruit(10, 1)\n\ndonnees = donneesSimples(300, f)\n\ndonnees_train_X, donnees_test_X, donnees_train_Y, donnees_test_Y = creerBases(donnees)\n\nneigh = skn.KNeighborsClassifier(n_neighbors=3)\nneigh.fit(donnees_train_X, donnees_train_Y)\n\ncolorationPlan(neigh, 50)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"....   \nCe code génère une fonction de séparation afin de créer un échantillon de données comportant du \"bruit\" en ajoutant donc des données à notre échantillon.","metadata":{}},{"cell_type":"markdown","source":"## Génération des données\nUtiliser le code précédent pour générer 500 données bruitées, et analyser à l'aide de graphiques l'effet du paramètre ```et```","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Analyser l'effet de la force du bruit sur les meilleurs paramètres du modèle","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Conclure, et vérifier l'importance des autres paramètres","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"---\n# Influence des corrélations entre les attributs\n---\n    On travaille à nouveau sur une base de onnées non bruitée. Ajouter 5 colonnes définies par :\n- U1 :   $+30 \\times X + 0.1 \\times Y$\n- U2 :   $-400 \\times X - 0.01 \\times Y$\n- U3 :   $-70 \\times X + 0.12 \\times Y$\n- U4 :   $-28 \\times X + 1.5 \\times Y$\n- U3 :   $+510 \\times X - 1.2 \\times Y$  \n\nRemettre la colonne ```classe``` en dernière colonne.","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Observer la diférence de qualité entre :\n- Apprendre en utilisant toutes les observations (x, y, u1, u2, u3, u4, u5)\n- Apprendre en n'utilisant que les observations (x,y)","metadata":{}},{"cell_type":"code","source":"\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n## Expliquer la raison de la différence observée","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"## Utiliser un preprocessing adapté sur les données afin que la qualité obtenue en apprenant sur le jeu de données (x, y, u1, u2, u3, u4, u5) soit comparable à celle obtenue en n'utilisant que (x, y)... expliquer\n","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"---\n# Merci d'avoir suivi ce TP, j'espère qu'il vous a aidé à mieux appréhender l'utilisation de knn, et vous a permis de faire quelques pas dans le domaine *passionnant* de l'apprentissage artificiel","metadata":{}}]}